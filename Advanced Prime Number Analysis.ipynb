{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk from 2 to 5000002\n",
      "Processing chunk from 5000002 to 10000002\n",
      "Processing chunk from 10000002 to 15000002\n",
      "Processing chunk from 15000002 to 20000002\n",
      "Processing chunk from 20000002 to 25000002\n",
      "Processing chunk from 25000002 to 30000002\n",
      "Processing chunk from 30000002 to 35000002\n",
      "Processing chunk from 35000002 to 40000002\n",
      "Processing chunk from 40000002 to 45000002\n",
      "Processing chunk from 45000002 to 50000002\n",
      "Processing chunk from 50000002 to 55000002\n",
      "Processing chunk from 55000002 to 60000002\n",
      "Processing chunk from 60000002 to 65000002\n",
      "Processing chunk from 65000002 to 70000002\n",
      "Processing chunk from 70000002 to 75000002\n",
      "Processing chunk from 75000002 to 80000002\n",
      "Processing chunk from 80000002 to 85000002\n",
      "Processing chunk from 85000002 to 90000002\n",
      "Processing chunk from 90000002 to 95000002\n",
      "Processing chunk from 95000002 to 100000002\n",
      "Processing chunk from 100000002 to 105000002\n",
      "Processing chunk from 105000002 to 110000002\n",
      "Processing chunk from 110000002 to 115000002\n",
      "Processing chunk from 115000002 to 120000002\n",
      "Processing chunk from 120000002 to 125000002\n",
      "Processing chunk from 125000002 to 130000002\n",
      "Processing chunk from 130000002 to 135000002\n",
      "Processing chunk from 135000002 to 140000002\n",
      "Processing chunk from 140000002 to 145000002\n",
      "Processing chunk from 145000002 to 150000002\n",
      "Processing chunk from 150000002 to 155000002\n",
      "Processing chunk from 155000002 to 160000002\n",
      "Processing chunk from 160000002 to 165000002\n",
      "Processing chunk from 165000002 to 170000002\n",
      "Processing chunk from 170000002 to 175000002\n",
      "Processing chunk from 175000002 to 180000002\n",
      "Processing chunk from 180000002 to 185000002\n",
      "Processing chunk from 185000002 to 190000002\n",
      "Processing chunk from 190000002 to 195000002\n",
      "Processing chunk from 195000002 to 200000002\n",
      "Processing chunk from 200000002 to 205000002\n",
      "Processing chunk from 205000002 to 210000002\n",
      "Processing chunk from 210000002 to 215000002\n",
      "Processing chunk from 215000002 to 220000002\n",
      "Processing chunk from 220000002 to 225000002\n",
      "Processing chunk from 225000002 to 230000002\n",
      "Processing chunk from 230000002 to 235000002\n",
      "Processing chunk from 235000002 to 240000002\n",
      "Processing chunk from 240000002 to 245000002\n",
      "Processing chunk from 245000002 to 250000002\n",
      "Processing chunk from 250000002 to 255000002\n",
      "Processing chunk from 255000002 to 260000002\n",
      "Processing chunk from 260000002 to 265000002\n",
      "Processing chunk from 265000002 to 270000002\n",
      "Processing chunk from 270000002 to 275000002\n",
      "Processing chunk from 275000002 to 280000002\n",
      "Processing chunk from 280000002 to 285000002\n",
      "Processing chunk from 285000002 to 290000002\n",
      "Processing chunk from 290000002 to 295000002\n",
      "Processing chunk from 295000002 to 300000002\n",
      "Processing chunk from 300000002 to 305000002\n",
      "Processing chunk from 305000002 to 310000002\n",
      "Processing chunk from 310000002 to 315000002\n",
      "Processing chunk from 315000002 to 320000002\n",
      "Processing chunk from 320000002 to 325000002\n",
      "Processing chunk from 325000002 to 330000002\n",
      "Processing chunk from 330000002 to 335000002\n",
      "Processing chunk from 335000002 to 340000002\n",
      "Processing chunk from 340000002 to 345000002\n",
      "Processing chunk from 345000002 to 350000002\n",
      "Processing chunk from 350000002 to 355000002\n",
      "Processing chunk from 355000002 to 360000002\n",
      "Processing chunk from 360000002 to 365000002\n",
      "Processing chunk from 365000002 to 370000002\n",
      "Processing chunk from 370000002 to 375000002\n",
      "Processing chunk from 375000002 to 380000002\n",
      "Processing chunk from 380000002 to 385000002\n",
      "Processing chunk from 385000002 to 390000002\n",
      "Processing chunk from 390000002 to 395000002\n",
      "Processing chunk from 395000002 to 400000002\n",
      "Processing chunk from 400000002 to 405000002\n",
      "Processing chunk from 405000002 to 410000002\n",
      "Processing chunk from 410000002 to 415000002\n",
      "Processing chunk from 415000002 to 420000002\n",
      "Processing chunk from 420000002 to 425000002\n",
      "Processing chunk from 425000002 to 430000002\n",
      "Processing chunk from 430000002 to 435000002\n",
      "Processing chunk from 435000002 to 440000002\n",
      "Processing chunk from 440000002 to 445000002\n",
      "Processing chunk from 445000002 to 450000002\n",
      "Processing chunk from 450000002 to 455000002\n",
      "Processing chunk from 455000002 to 460000002\n",
      "Processing chunk from 460000002 to 465000002\n",
      "Processing chunk from 465000002 to 470000002\n",
      "Processing chunk from 470000002 to 475000002\n",
      "Processing chunk from 475000002 to 480000002\n",
      "Processing chunk from 480000002 to 485000002\n",
      "Processing chunk from 485000002 to 490000002\n",
      "Processing chunk from 490000002 to 495000002\n",
      "Processing chunk from 495000002 to 500000002\n",
      "Processing chunk from 500000002 to 505000002\n",
      "Processing chunk from 505000002 to 510000002\n",
      "Processing chunk from 510000002 to 515000002\n",
      "Processing chunk from 515000002 to 520000002\n",
      "Processing chunk from 520000002 to 525000002\n",
      "Processing chunk from 525000002 to 530000002\n",
      "Processing chunk from 530000002 to 535000002\n",
      "Processing chunk from 535000002 to 540000002\n",
      "Processing chunk from 540000002 to 545000002\n",
      "Processing chunk from 545000002 to 550000002\n",
      "Processing chunk from 550000002 to 555000002\n",
      "Processing chunk from 555000002 to 560000002\n",
      "Processing chunk from 560000002 to 565000002\n",
      "Processing chunk from 565000002 to 570000002\n",
      "Processing chunk from 570000002 to 575000002\n",
      "Processing chunk from 575000002 to 580000002\n",
      "Processing chunk from 580000002 to 585000002\n",
      "Processing chunk from 585000002 to 590000002\n",
      "Processing chunk from 590000002 to 595000002\n",
      "Processing chunk from 595000002 to 600000002\n",
      "Processing chunk from 600000002 to 605000002\n",
      "Processing chunk from 605000002 to 610000002\n",
      "Processing chunk from 610000002 to 615000002\n",
      "Processing chunk from 615000002 to 620000002\n",
      "Processing chunk from 620000002 to 625000002\n",
      "Processing chunk from 625000002 to 630000002\n",
      "Processing chunk from 630000002 to 635000002\n",
      "Processing chunk from 635000002 to 640000002\n",
      "Processing chunk from 640000002 to 645000002\n",
      "Processing chunk from 645000002 to 650000002\n"
     ]
    }
   ],
   "source": [
    "from sympy import primerange, log\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "def generate_primes_in_chunk(start, end):\n",
    "    \"\"\"\n",
    "    Generate primes in a given range using a generator to save memory.\n",
    "    \"\"\"\n",
    "    return primerange(start, end)\n",
    "\n",
    "def compute_primes_in_chunks(limit, chunk_size=5000000, sub_chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Compute primes, twin primes, and gaps in chunks to manage memory.\n",
    "    Saves progress frequently to disk.\n",
    "    \"\"\"\n",
    "    all_primes = []\n",
    "    twin_primes = []\n",
    "    gap_counts = {8: [], 10: [], 12: [], 14: []}\n",
    "    primes_class_1 = []\n",
    "    primes_class_5 = []\n",
    "    \n",
    "    for start in range(2, limit, chunk_size):\n",
    "        end = min(start + chunk_size, limit)\n",
    "        print(f\"Processing chunk from {start} to {end}\")\n",
    "        \n",
    "        for i in range(start, end, sub_chunk_size):\n",
    "            sub_end = min(i + sub_chunk_size, end)\n",
    "            chunk_primes = list(generate_primes_in_chunk(i, sub_end))\n",
    "            all_primes.extend(chunk_primes)\n",
    "            \n",
    "            chunk_primes_set = set(chunk_primes)\n",
    "            twin_primes.extend([(p, p + 2) for p in chunk_primes if p % 6 == 5 and (p + 2) in chunk_primes_set])\n",
    "            primes_class_1.extend([p for p in chunk_primes if p % 6 == 1])\n",
    "            primes_class_5.extend([p for p in chunk_primes if p % 6 == 5])\n",
    "            \n",
    "            for gap in [8, 10, 12, 14]:\n",
    "                gap_counts[gap].extend([(p, p + gap) for p in chunk_primes if (p + gap) in chunk_primes_set])\n",
    "            \n",
    "            # Save progress after each sub-chunk\n",
    "            with open(f'progress_chunk_{i}.pkl', 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'all_primes': all_primes,\n",
    "                    'twin_primes': twin_primes,\n",
    "                    'primes_class_1': primes_class_1,\n",
    "                    'primes_class_5': primes_class_5,\n",
    "                    'gap_counts': gap_counts\n",
    "                }, f)\n",
    "            gc.collect()  # Clean up memory\n",
    "    \n",
    "    return all_primes, twin_primes, primes_class_1, primes_class_5, gap_counts\n",
    "\n",
    "def advanced_prime_analysis(limit, chunk_size=5000000, sub_chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Main analysis function, resuming from saved progress or starting fresh.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to load the latest progress file\n",
    "        latest_chunk = max([int(f.split('_')[2].split('.')[0]) for f in os.listdir('.') if f.startswith('progress_chunk_') and f.endswith('.pkl')], default=1)\n",
    "        with open(f'progress_chunk_{latest_chunk}.pkl', 'rb') as f:\n",
    "            progress = pickle.load(f)\n",
    "            all_primes = progress['all_primes']\n",
    "            twin_primes = progress['twin_primes']\n",
    "            primes_class_1 = progress['primes_class_1']\n",
    "            primes_class_5 = progress['primes_class_5']\n",
    "            gap_counts = progress['gap_counts']\n",
    "        \n",
    "        # Find the last prime processed to resume from there\n",
    "        last_prime = max(all_primes) if all_primes else 1\n",
    "        next_start = last_prime + 1\n",
    "        \n",
    "        # Continue computation from where we left off\n",
    "        new_all_primes, new_twin_primes, new_primes_class_1, new_primes_class_5, new_gap_counts = compute_primes_in_chunks(limit, chunk_size, sub_chunk_size, start_from=next_start)\n",
    "        \n",
    "        # Combine results\n",
    "        all_primes.extend(new_all_primes)\n",
    "        twin_primes.extend(new_twin_primes)\n",
    "        primes_class_1.extend(new_primes_class_1)\n",
    "        primes_class_5.extend(new_primes_class_5)\n",
    "        for gap in [8, 10, 12, 14]:\n",
    "            gap_counts[gap].extend(new_gap_counts[gap])\n",
    "        \n",
    "    except (FileNotFoundError, ValueError):\n",
    "        # If no progress file exists or error occurs, start from scratch\n",
    "        all_primes, twin_primes, primes_class_1, primes_class_5, gap_counts = compute_primes_in_chunks(limit, chunk_size, sub_chunk_size)\n",
    "    \n",
    "    # Density Analysis\n",
    "    C = 1.3203239  # Hardy-Littlewood constant for twin primes\n",
    "    theoretical_density_twins = C * limit / (math.log(limit)) ** 2\n",
    "    empirical_density_twins = len(twin_primes) / math.log(limit)\n",
    "    \n",
    "    print(f\"Empirical Density of Twin Primes: {empirical_density_twins}\")\n",
    "    print(f\"Theoretical Density of Twin Primes: {theoretical_density_twins}\")\n",
    "    print(f\"Ratio (Empirical/Theoretical): {empirical_density_twins / theoretical_density_twins}\")\n",
    "\n",
    "    print(f\"\\nNumber of primes in class 1: {len(primes_class_1)}\")\n",
    "    print(f\"Number of primes in class 5: {len(primes_class_5)}\")\n",
    "    \n",
    "    # Exploration of Other Gaps\n",
    "    for gap in [8, 10, 12, 14]:\n",
    "        print(f\"\\nPrimes with gap {gap} up to {limit}: {gap_counts[gap][:10]}...\")\n",
    "        print(f\"Number of prime pairs with gap {gap}: {len(gap_counts[gap])}\")\n",
    "\n",
    "    # Statistical Analysis\n",
    "    total_primes = len(primes_class_1) + len(primes_class_5)\n",
    "    expected = total_primes / 2\n",
    "    chi2 = ((len(primes_class_1) - expected) ** 2 / expected) + ((len(primes_class_5) - expected) ** 2 / expected)\n",
    "    \n",
    "    print(f\"\\nChi-square Statistic: {chi2}\")\n",
    "    print(f\"Note: For chi2 < 3.84 (1 df, p > 0.05), distribution is consistent with equal split\")\n",
    "    print(f\"Conclusion: {'Consistent with equal distribution' if chi2 < 3.84 else 'Possible deviation'}\")\n",
    "    \n",
    "    # Plotting\n",
    "    gaps = [8, 10, 12, 14]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(gaps, [len(gap_counts[gap]) for gap in gaps], 'o-')\n",
    "    plt.title('Number of Prime Pairs vs. Gap Size')\n",
    "    plt.xlabel('Gap Size')\n",
    "    plt.ylabel('Number of Pairs')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Set the limit\n",
    "limit = 10000000000  # 10^10\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    advanced_prime_analysis(limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.0",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
